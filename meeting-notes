meeting notes:

variational bayes overview: https://people.inf.ethz.ch/bkay/talks/Brodersen_2013_03_22.pdf

1/18 time series meeting:
andreas: hierarchical models
implement vRNN, deep kalman filter, compare to pauls results
see perf improvement below

---

Notes talking to paul szerlip 1/31:
print doesnt work, needs to be implemented
ideally: one cmake file that custom builds torch inside adnn
under 10k not faster than js, ffi calls too expensive. use node js addons, use node v8 to call torch C functions, much faster
torch: library + lua library. binding on the lua layer
might be worth running on images. 
no speedup between lua and js, both single threaded. run on different matrix lengths
problem: referencing and dereferencing: sum elements with all filled 1, see if add up. 

1/11/17 meeting: 

time series - language? 
building go in adnn/webppl CUDA? stanford cluster or ec2 

research thing? some diff models: deep kalman filter, variational rnn, var autoencoder
replicate results, implement model and run it on their data
apply deep kalman  to language 
SEE: vrnn paper: https://arxiv.org/pdf/1506.02216v6.pdf

RSA stuff: 
what people do: train literal model, stick RSA on top of it
what you shoudl do: take RSA, adjust the semantics (meaning) to match the data, hard to do because oyu need to backprop data

---
Talk with paul:
explain the nn as functions hes doing
does it/how does it apply to the swapping out adnn backedn that im doing, will there be webppl code change or just adnn
loko at slack discussion (couple bugs he fixed)
explain documentation for samples issue ()
advice for starting variational inference on time series data
what other tasks? (see below)

nn is a fn that uses those params, dot product of vector. nn in adnn doesnt have backend in webppl to create params. it needs to keep the state (weight matrices), see tech reqport. `nneval` not in webppl but in tech report - call the funcatin with params

time series: variational inf 
pauls writing guide programs so posterior can capture correlations. guide part is recurrent.
implement kalman filter, get smc working

laplace distr: https://github.com/probmods/webppl/issues/672
issues label in inferences: optimized params (658). function would Take param to say which group of params to optimize. keep track of which params seen 


webppl:
david mckai? info theory and algos
chris bishop, kevin murphy
coursera graphical models
adnn - backend "automatic differentiation"
implmentation of guides
distributions are just js objects with md
laplace distr: https://github.com/probmods/webppl/issues/672 -> reparameterization trick, restructure so AD flows better. distr implements this trick. instead of sampling from distr, sample from a normal dist, then transform it deterministaclly. base is a fixed dist you sample from, transform transforms it to the one in quesiton
issues label in inferences: optimized params (658). function would Take param to say which group of params to optimize. keep track of which params seen when program executed and whichs hould be optimized
https://github.com/probmods/webppl/issues/530
look up: metropolis hastings, particle filtering


12/7

Pragmatics, RSA - rational speech acts model
remove need to build semantics by hand, learn semantics -> plug to RSA -> predict human usage
watch task where two people chat via window, describe actions to someone (pick a box)
katherine, robert presented
relies on deep amortized inference



smaller projects:
consult: daniel, paul
benchmarking methods
swapping otu tensor backend for somethign more efficient - (best ways tp do that?)
bind directly to torch backend, node ffi 
efficiency of tensorflow? compared to torch (do some research on this, bind to tf directly? is it computationally fit model-wise: can we indiivudllay call kernels, instea dof computing graph then running). torch is more similar - someone wrote js bindings for torch
ADNN:
tensor - swap this for C library instead of JS array (binded libTH, libTHC(Cuda), libTHNN libTHCUNN (Cuda)-ec2 or use stanford lab)
ad - builds forward graph of computations for backprop (try to change as little as possible)
nn- build nn objects. evaluate(), stores params (dont have to worry about) <- paul is working on a replacement
opt - optimizatin methods (no change)

correctness test
speed testbed - 
matrix multioplies
fully connected convultion nn

1. look at geometric intelligence code (dont change webppl code, just bind tensor layer)
2. point it to libTH (std torch libraries where they used their own)
3. find what methods are useful
3. cuda support 
4. make backend togglable - js tensor, cuda tensor (do for v0)

adnn:
ad: derivs, functions.js
nn: activation.js move relu to ad (ad primitives)

meeting 1/16:

fixed control flow -> cache whole graph
dan learning towards object pool. everytime you rerun you should find the same node you need to use. 
pool of Tensor nodes and scalar nodes.
hash that was keyed on dimension of tensor. 
you want to run a tensor: input dim and output dim is this. give me node. if you dont find exact match, get new one 
options for clearing the cache and cap it.
-- when you run oopt in webppl you get a node and thats the root. after you take a gradient step, you want to be able to call on that node something like "release nodes to pool". if it is global you can release all nodes wihtout retraversing the graph (the call stack).

make go completely in js using adnn. diff between adnn and webppl-nn?
todo:
make togglable: kind of like torchnn impl. 
test benchmarks: convolution and matrx tensors should be the big improvements. really big perceptron or convnet 
cuda
move nn things ad primitive (see comments in the code, pooling relu conv)

Perf IMprovement! 
where to spend effort for improvement step up?
consult: paul, jason, daniel
profile what resources being used, document.  using node7 and chrome, in the docs.
gc, tensor backend, or something else
 if model with no control flow change -> reuse AD graph (same tensor on each iteration)

=====
time series:
https://arxiv.org/pdf/1605.06432v2.pdf
hmm - discrete
 filter- continuous
 using autoencoders - deep common filter: filter.  transition functio is deepnet LSTM-like. RNN. recognition model that learns inference. - can write in webppl. will it work?

try running inference on time series models - how well does it work and scale?
implement kalman filter, get smc working
consult: paul, ndg
evolution of a ecosystem, financial modeling, baseball
